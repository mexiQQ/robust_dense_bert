Namespace(adam_epsilon=1e-08, bias_correction=True, bsz=32, ckpt_dir=PosixPath('sparse/outputs2'), dataset_name='glue', debug=False, do_lower_case=True, epochs=5, eval_size=32, force_overwrite='1', lr=2e-05, max_seq_length=128, model_name='/hdd1/jianwei/workspace/OBC/bert/outputs_2x_3', num_examples=872, num_labels=2, result_file='attack_result.csv', seed=42, task_name='sst2', warmup_ratio=0.1, weight_decay=0.01)
Making checkpoint directory: sparse/outputs2/finetune_glue-sst2_lr2e-05_epochs5_seed42_time1683247966961
bert.encoder.layer.0.attention.self.query 0.4983198377821181
bert.encoder.layer.0.attention.self.key 0.49741787380642366
bert.encoder.layer.0.attention.self.value 0.49778408474392366
bert.encoder.layer.0.attention.output.dense 0.49812825520833337
bert.encoder.layer.0.intermediate.dense 0.4985182020399306
bert.encoder.layer.0.output.dense 0.49953926934136283
bert.encoder.layer.1.attention.self.query 0.49792989095052087
bert.encoder.layer.1.attention.self.key 0.49781460232204866
bert.encoder.layer.1.attention.self.value 0.4978095160590278
bert.encoder.layer.1.attention.output.dense 0.49815707736545134
bert.encoder.layer.1.intermediate.dense 0.4986271328396268
bert.encoder.layer.1.output.dense 0.4995274013943143
bert.encoder.layer.2.attention.self.query 0.49796549479166663
bert.encoder.layer.2.attention.self.key 0.49789089626736116
bert.encoder.layer.2.attention.self.value 0.4979400634765625
bert.encoder.layer.2.attention.output.dense 0.4981452094184028
bert.encoder.layer.2.intermediate.dense 0.4986614651150174
bert.encoder.layer.2.output.dense 0.49954096476236975
bert.encoder.layer.3.attention.self.query 0.49802144368489587
bert.encoder.layer.3.attention.self.key 0.497833251953125
bert.encoder.layer.3.attention.self.value 0.4978179931640625
bert.encoder.layer.3.attention.output.dense 0.49807400173611116
bert.encoder.layer.3.intermediate.dense 0.4986835055881076
bert.encoder.layer.3.output.dense 0.49954350789388025
bert.encoder.layer.4.attention.self.query 0.49812825520833337
bert.encoder.layer.4.attention.self.key 0.4978400336371528
bert.encoder.layer.4.attention.self.value 0.49773830837673616
bert.encoder.layer.4.attention.output.dense 0.4983300103081597
bert.encoder.layer.4.intermediate.dense 0.49868435329861116
bert.encoder.layer.4.output.dense 0.4995070563422309
bert.encoder.layer.5.attention.self.query 0.49821133083767366
bert.encoder.layer.5.attention.self.key 0.49798583984375
bert.encoder.layer.5.attention.self.value 0.4976874457465278
bert.encoder.layer.5.attention.output.dense 0.49842495388454866
bert.encoder.layer.5.intermediate.dense 0.49868181016710067
bert.encoder.layer.5.output.dense 0.4995223151312934
bert.encoder.layer.6.attention.self.query 0.49817911783854163
bert.encoder.layer.6.attention.self.key 0.49811299641927087
bert.encoder.layer.6.attention.self.value 0.49774509006076384
bert.encoder.layer.6.attention.output.dense 0.4983283148871528
bert.encoder.layer.6.intermediate.dense 0.49869071112738717
bert.encoder.layer.6.output.dense 0.4995536804199219
bert.encoder.layer.7.attention.self.query 0.4982215033637153
bert.encoder.layer.7.attention.self.key 0.4981553819444444
bert.encoder.layer.7.attention.self.value 0.49769931369357634
bert.encoder.layer.7.attention.output.dense 0.49825710720486116
bert.encoder.layer.7.intermediate.dense 0.49868223402235246
bert.encoder.layer.7.output.dense 0.4995557996961806
bert.encoder.layer.8.attention.self.query 0.49824863009982634
bert.encoder.layer.8.attention.self.key 0.4982011583116319
bert.encoder.layer.8.attention.self.value 0.49771457248263884
bert.encoder.layer.8.attention.output.dense 0.49820454915364587
bert.encoder.layer.8.intermediate.dense 0.49866951836480033
bert.encoder.layer.8.output.dense 0.4995405409071181
bert.encoder.layer.9.attention.self.query 0.49828084309895837
bert.encoder.layer.9.attention.self.key 0.4982757568359375
bert.encoder.layer.9.attention.self.value 0.49782816569010413
bert.encoder.layer.9.attention.output.dense 0.49806552463107634
bert.encoder.layer.9.intermediate.dense 0.4986610412597656
bert.encoder.layer.9.output.dense 0.49955622355143225
bert.encoder.layer.10.attention.self.query 0.49844699435763884
bert.encoder.layer.10.attention.self.key 0.49812655978732634
bert.encoder.layer.10.attention.self.value 0.49778408474392366
bert.encoder.layer.10.attention.output.dense 0.49810451931423616
bert.encoder.layer.10.intermediate.dense 0.49864535861545134
bert.encoder.layer.10.output.dense 0.49957784016927087
bert.encoder.layer.11.attention.self.query 0.4985334608289931
bert.encoder.layer.11.attention.self.key 0.49808926052517366
bert.encoder.layer.11.attention.self.value 0.4980180528428819
bert.encoder.layer.11.attention.output.dense 0.4980248345269097
bert.encoder.layer.11.intermediate.dense 0.4985783894856771
bert.encoder.layer.11.output.dense 0.49957784016927087
bert.pooler.dense 0.0
classifier 0.0
Epoch: 0, Loss:  0.1471, Lr:  1.778e-05, Dev_Accuracy: 0.8944954128440366
**** Test Accuracy: 0.8944954128440366, Test_Loss: 0.40088702259319026
Epoch: 1, Loss:  0.0568, Lr:  1.333e-05, Dev_Accuracy: 0.9048165137614678
**** Test Accuracy: 0.9048165137614678, Test_Loss: 0.3500177030052445
Epoch: 2, Loss:  0.0353, Lr:  8.886e-06, Dev_Accuracy: 0.900229357798165
Epoch: 3, Loss:  0.0223, Lr:  4.441e-06, Dev_Accuracy: 0.896788990825688
Epoch: 4, Loss:  0.0139, Lr:  0.000e+00, Dev_Accuracy: 0.8979357798165136
**** Best dev metric: 0.9048165137614678 in Epoch: 1
**** Best Test metric: 0.9048165137614678 in Epoch: 1
Last epoch test_accuracy: 0.8979357798165136, test_loss: 0.559548107907174
Spend time: 0.27055555555555555
